{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cifar10-in-notebook-download.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "YFJu_N0QfxZ0"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "Byrx7AK7Mr06",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "U2rAoSMcPzo3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "num_class = 10\n",
        "image_height = 32\n",
        "image_width = 32\n",
        "num_training_instance = 50000\n",
        "num_test_instance = 10000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZHnb51yqM-PY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "training_data, test_data = tf.keras.datasets.cifar10.load_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "apNibQpjN9FU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x_train = training_data[0]\n",
        "y_train = training_data[1]\n",
        "x_test = test_data[0]\n",
        "y_test = test_data[1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "R84BTOSuOdTr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e839d04a-b4a9-45b0-acb6-f8591bd68009"
      },
      "cell_type": "code",
      "source": [
        "x_train.shape"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(50000, 32, 32, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "metadata": {
        "id": "YJ9hZ8iROfmR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ed718877-d55a-423a-b50e-9e8f6b217ba8"
      },
      "cell_type": "code",
      "source": [
        "y_train.shape"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(50000, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "metadata": {
        "id": "mmJ93hV8PAhv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# need to convert to one hot vectors\n",
        "def get_one_hot_labels(normal_label, num_class):\n",
        "  num_labels = normal_label.shape[0]\n",
        "  one_hot_label = np.zeros([num_labels, num_class])\n",
        "  for i in range(num_labels):\n",
        "    current_index = normal_label[i]\n",
        "    one_hot_label[i][current_index] += 1\n",
        "  return one_hot_label"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Sa9zU96LQkCG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "y_train_one_hot = get_one_hot_labels(y_train, num_class)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "w3OLHio0QqmM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bbfc89e3-95e2-40e5-85bb-d90d51f3b165"
      },
      "cell_type": "code",
      "source": [
        "y_train_one_hot.shape"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(50000, 10)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "metadata": {
        "id": "v5Vq2wm7QsBo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "y_test_one_hot = get_one_hot_labels(y_test, num_class)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IplWnxIcQxp9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4f30bb05-5a09-4f6b-fa61-5099239d4779"
      },
      "cell_type": "code",
      "source": [
        "y_test_one_hot.shape"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 10)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "metadata": {
        "id": "BGvIka1xQy5v",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Prepare tf data for input into model\n",
        "\n",
        "## setting some training variables\n",
        "\n",
        "batch_size = 64\n",
        "\n",
        "num_train_iteration_per_epoch = num_training_instance // batch_size\n",
        "if num_training_instance % batch_size != 0:\n",
        "  num_train_iteration_per_epoch += 1\n",
        "  \n",
        "num_test_iteration_per_epoch = num_test_instance // batch_size\n",
        "if num_test_instance % batch_size != 0:\n",
        "  num_test_iteration_per_epoch += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dqwNE8LMRo9k",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# dataset_training = tf.data.Dataset.from_tensor_slices((x_train, y_train_one_hot)).batch(batch_size).repeat()\n",
        "dataset_training = tf.data.Dataset.from_tensor_slices((x_train, y_train_one_hot)).shuffle(1000).batch(batch_size).repeat()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZQxvTsQwdmXz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# going to create one more set of training dataset that is not batched but for testing training accuracy\n",
        "# dataset_training_non_batch = tf.data.Dataset.from_tensor_slices((x_train, y_train_one_hot)).batch(num_training_instance).repeat()\n",
        "\n",
        "# going to do a running metric instead of 1 shot evaluation of metrics.\n",
        "# dataset_training_non_batch = tf.data.Dataset.from_tensor_slices((x_train, y_train_one_hot)).shuffle(1000).batch(num_training_instance).repeat()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BVhLNHyaTEWF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# might change this one to a running test metric too\n",
        "dataset_test = tf.data.Dataset.from_tensor_slices((x_test, y_test_one_hot)).batch(batch_size).repeat()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "L8iRNwRnTMXh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "iterator = tf.data.Iterator.from_structure(dataset_training.output_types, dataset_training.output_shapes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EN3NjOzyVG5I",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_init = iterator.make_initializer(dataset_training)\n",
        "test_init = iterator.make_initializer(dataset_test)\n",
        "\n",
        "# train_whole_init = iterator.make_initializer(dataset_training_non_batch)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lcjk4FTfVRm8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "image_batch, label_batch = iterator.get_next()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jbDe3LXm3Xgj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# setting up the metrics --- accuracy and mean(for running cost)\n",
        "# tf_accuracy, tf_accuracy_update = tf.metrics.accuracy(y_predicted, label_batch_stopped)\n",
        "\n",
        "# tf_cost, tf_cost_update = tf.metrics.mean(cost)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eP9k0ozcV_fd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# dropouts suppose to be 0.3, 0.3, 0.6, 0.6\n",
        "\n",
        "######Need to use a placeholder to control dropout during training and turn off during evaluation, by default dropout is disabled, thats why overfitting\n",
        "is_training_mode = tf.placeholder_with_default(True, shape=())\n",
        "\n",
        "# write model\n",
        "\n",
        "input_cast_layer = tf.cast(image_batch, tf.float32)\n",
        "\n",
        "conv1 = tf.layers.conv2d(input_cast_layer, filters=64, kernel_size=[3, 3], strides=1, activation='relu', padding='same', name='conv_layer_1')\n",
        "\n",
        "# somehow things got from stagnanting at 50% accuracy for test set to close to 68 percent accuracy when i change stride from 1 to 2??? why??\n",
        "pool1 = tf.layers.max_pooling2d(conv1, pool_size=[2, 2], strides=2, padding='same', name='max_pool_layer_1')\n",
        "\n",
        "batch_norm1 = tf.layers.batch_normalization(pool1, name='batch_norm_1')\n",
        "# batch_norm1 = pool1\n",
        "\n",
        "conv_drop1 = tf.layers.dropout(batch_norm1, rate=0.3, seed=22, training=is_training_mode, name='conv_drop_1')\n",
        "\n",
        "conv2 = tf.layers.conv2d(conv_drop1, filters=64, kernel_size=[3, 3], strides=1, activation='relu', padding='same', name='conv_layer_2')\n",
        "\n",
        "pool2 = tf.layers.max_pooling2d(conv2, pool_size=[2, 2], strides=[2, 2], padding='same', name='max_pool_layer_2')\n",
        "\n",
        "batch_norm2 = tf.layers.batch_normalization(pool2, name='batch_norm_2')\n",
        "# batch_norm2 = pool2\n",
        "conv_drop2 = tf.layers.dropout(batch_norm2, rate=0.4, seed=25, training=is_training_mode, name='conv_drop_2')\n",
        "\n",
        "flat_layer = tf.layers.flatten(batch_norm2)\n",
        "\n",
        "dense0 = tf.layers.dense(flat_layer, units=256, activation='relu', name='dense_layer_0')\n",
        "\n",
        "dropout0 = tf.layers.dropout(dense0, rate=0.6, seed=1, training=is_training_mode, name='dropout_layer_0')\n",
        "\n",
        "dense1 = tf.layers.dense(dropout0, units=128, activation='relu', name='dense_layer_1')\n",
        "\n",
        "dropout = tf.layers.dropout(dense1, rate=0.6, seed=42, training=is_training_mode, name='dropout_layer_1')\n",
        "######## before this dense2 is a softmax, which hindered the learning, because softmax cross entropy applied softmax again, breaking the learning\n",
        "######## https://stackoverflow.com/questions/50032197/tensorflow-gradients-are-0-weights-are-not-updating\n",
        "\n",
        "dense2 = tf.layers.dense(dropout, units=num_class, activation=None, name='dense_layer_2_softmax_output')\n",
        "\n",
        "# dense2 = tf.layers.dense(dropout, units=num_class, activation='relu', name='dense_layer_2_softmax_output')\n",
        "\n",
        "# defining the output of the model\n",
        "\n",
        "y_predicted = dense2\n",
        "\n",
        "y_predicted_class = tf.argmax(y_predicted, axis=1)\n",
        "\n",
        "# stopping backprop to gradients\n",
        "label_batch_stopped = tf.stop_gradient(label_batch)\n",
        "\n",
        "# getting ground truth label class indices\n",
        "label_class = tf.argmax(label_batch_stopped, axis=1)\n",
        "\n",
        "cost_function = tf.nn.softmax_cross_entropy_with_logits_v2(labels=label_batch_stopped, logits=dense2)\n",
        "\n",
        "cost = tf.reduce_mean(cost_function)\n",
        "\n",
        "optimizer = tf.train.AdamOptimizer()\n",
        "\n",
        "train = optimizer.minimize(cost)\n",
        "\n",
        "# defining metrics\n",
        "\n",
        "tf_accuracy, tf_accuracy_update = tf.metrics.accuracy(y_predicted_class, label_class, name='accuracy_metrics')\n",
        "\n",
        "tf_cost, tf_cost_update = tf.metrics.mean(cost, name='cost_metrics')\n",
        "\n",
        "\n",
        "# Not using the ones below\n",
        "# correct_prediction = tf.equal(y_predicted_class, tf.argmax(label_batch, axis=1))\n",
        "\n",
        "# accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "# accuracy = tf.metrics.accuracy(y_predicted, label_batch)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HZTGY0Xs7fc7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# getting the running variables for accuracy and cost metrics\n",
        "accuracy_vars = tf.get_collection(tf.GraphKeys.LOCAL_VARIABLES, scope='accuracy_metrics')\n",
        "cost_vars = tf.get_collection(tf.GraphKeys.LOCAL_VARIABLES, scope='cost_metrics')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jeip6-OTeV9w",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sess = tf.Session()\n",
        "sess.run(tf.global_variables_initializer())\n",
        "sess.run(tf.local_variables_initializer())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gQplea5-zT_1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# sess.run(tf.variables_initializer(accuracy_vars))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GeTROI3g1NUj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "training_feed_dict = {is_training_mode : True}\n",
        "testing_feed_dict = {is_training_mode : False}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NI0pvcL2hAc3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37417
        },
        "outputId": "14492723-4ec2-4dec-f1e9-d4e80fbd30c6"
      },
      "cell_type": "code",
      "source": [
        "# training with evaluation after every epoch\n",
        "\n",
        "num_epoch = 200\n",
        "\n",
        "for i in range(num_epoch):\n",
        "  print('\\n')\n",
        "  # print the current number of epoch\n",
        "  print('Epoch: ', i)\n",
        "  \n",
        "  # reset the metrics\n",
        "  sess.run(tf.variables_initializer(accuracy_vars))\n",
        "  sess.run(tf.variables_initializer(cost_vars))\n",
        "  \n",
        "  # initialize the training dataset\n",
        "  sess.run(train_init)\n",
        "  \n",
        "  # run the training iterations for 1 epoch\n",
        "  for j in range(num_train_iteration_per_epoch):\n",
        "    # for every batch\n",
        "    sess.run(train, feed_dict=training_feed_dict)\n",
        "    # keep track of accuracy and loss\n",
        "    sess.run([tf_accuracy_update, tf_cost_update])\n",
        "  \n",
        "  print('Training data')\n",
        "  print('Training loss: ', sess.run(tf_cost))\n",
        "  print('Training accuracy: ', sess.run(tf_accuracy))\n",
        "  \n",
        "  \n",
        "  # reset the metrics\n",
        "  sess.run(tf.variables_initializer(accuracy_vars))\n",
        "  sess.run(tf.variables_initializer(cost_vars))\n",
        "    \n",
        "  # initialize the test dataset\n",
        "  sess.run(test_init)\n",
        "  \n",
        "  for _ in range(num_test_iteration_per_epoch):\n",
        "    sess.run([cost, tf_cost_update, tf_accuracy_update], feed_dict=testing_feed_dict)\n",
        "  \n",
        "  print('\\n')\n",
        "  print('Test data')\n",
        "  # evaluate the current training progress over the test data and print them\n",
        "  ## take in input data\n",
        "#   sess.run([iterator.get_next()])\n",
        "  \n",
        "#   sess.run(tf.local_variables_initializer())\n",
        "  \n",
        "  print('Loss: ', sess.run(tf_cost))\n",
        "  \n",
        "  print('Accuracy: ', sess.run(tf_accuracy))\n",
        "  \n",
        "  ####### going to test the model on training to understand the reduction in accuracy as training proceeds\n",
        "  \n",
        "#   print('Training data')\n",
        "  \n",
        "#   sess.run(train_whole_init)\n",
        "  \n"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Epoch:  0\n",
            "Training data\n",
            "Training loss:  3.0012012\n",
            "Training accuracy:  0.12592088\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  2.2685301\n",
            "Accuracy:  0.1466\n",
            "\n",
            "\n",
            "Epoch:  1\n",
            "Training data\n",
            "Training loss:  2.2129273\n",
            "Training accuracy:  0.16768098\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  2.1460645\n",
            "Accuracy:  0.2181\n",
            "\n",
            "\n",
            "Epoch:  2\n",
            "Training data\n",
            "Training loss:  2.026087\n",
            "Training accuracy:  0.23182255\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  1.9039735\n",
            "Accuracy:  0.2868\n",
            "\n",
            "\n",
            "Epoch:  3\n",
            "Training data\n",
            "Training loss:  1.8665513\n",
            "Training accuracy:  0.29204035\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  1.7555264\n",
            "Accuracy:  0.3407\n",
            "\n",
            "\n",
            "Epoch:  4\n",
            "Training data\n",
            "Training loss:  1.763046\n",
            "Training accuracy:  0.33924568\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  1.6406157\n",
            "Accuracy:  0.3669\n",
            "\n",
            "\n",
            "Epoch:  5\n",
            "Training data\n",
            "Training loss:  1.6755065\n",
            "Training accuracy:  0.38298768\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  1.5239698\n",
            "Accuracy:  0.43\n",
            "\n",
            "\n",
            "Epoch:  6\n",
            "Training data\n",
            "Training loss:  1.6036227\n",
            "Training accuracy:  0.41493833\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  1.4554404\n",
            "Accuracy:  0.4659\n",
            "\n",
            "\n",
            "Epoch:  7\n",
            "Training data\n",
            "Training loss:  1.5367266\n",
            "Training accuracy:  0.4424247\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  1.4895744\n",
            "Accuracy:  0.4398\n",
            "\n",
            "\n",
            "Epoch:  8\n",
            "Training data\n",
            "Training loss:  1.4618585\n",
            "Training accuracy:  0.47049168\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  1.4006691\n",
            "Accuracy:  0.4808\n",
            "\n",
            "\n",
            "Epoch:  9\n",
            "Training data\n",
            "Training loss:  1.4107975\n",
            "Training accuracy:  0.49491513\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  1.2842762\n",
            "Accuracy:  0.529\n",
            "\n",
            "\n",
            "Epoch:  10\n",
            "Training data\n",
            "Training loss:  1.3505659\n",
            "Training accuracy:  0.52108026\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  1.2421813\n",
            "Accuracy:  0.5457\n",
            "\n",
            "\n",
            "Epoch:  11\n",
            "Training data\n",
            "Training loss:  1.3063165\n",
            "Training accuracy:  0.54107946\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  1.1947114\n",
            "Accuracy:  0.5708\n",
            "\n",
            "\n",
            "Epoch:  12\n",
            "Training data\n",
            "Training loss:  1.2628454\n",
            "Training accuracy:  0.55971736\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  1.1216285\n",
            "Accuracy:  0.5945\n",
            "\n",
            "\n",
            "Epoch:  13\n",
            "Training data\n",
            "Training loss:  1.2046776\n",
            "Training accuracy:  0.58159834\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  1.0962862\n",
            "Accuracy:  0.606\n",
            "\n",
            "\n",
            "Epoch:  14\n",
            "Training data\n",
            "Training loss:  1.1729914\n",
            "Training accuracy:  0.5935098\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  1.0838346\n",
            "Accuracy:  0.6079\n",
            "\n",
            "\n",
            "Epoch:  15\n",
            "Training data\n",
            "Training loss:  1.1344464\n",
            "Training accuracy:  0.61046606\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  1.0429578\n",
            "Accuracy:  0.6366\n",
            "\n",
            "\n",
            "Epoch:  16\n",
            "Training data\n",
            "Training loss:  1.1115292\n",
            "Training accuracy:  0.62017536\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  1.0302998\n",
            "Accuracy:  0.6399\n",
            "\n",
            "\n",
            "Epoch:  17\n",
            "Training data\n",
            "Training loss:  1.0790269\n",
            "Training accuracy:  0.6335282\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  1.0485244\n",
            "Accuracy:  0.6221\n",
            "\n",
            "\n",
            "Epoch:  18\n",
            "Training data\n",
            "Training loss:  1.0489925\n",
            "Training accuracy:  0.6405549\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  0.97110367\n",
            "Accuracy:  0.6576\n",
            "\n",
            "\n",
            "Epoch:  19\n",
            "Training data\n",
            "Training loss:  1.0215029\n",
            "Training accuracy:  0.6503443\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  0.9716469\n",
            "Accuracy:  0.6699\n",
            "\n",
            "\n",
            "Epoch:  20\n",
            "Training data\n",
            "Training loss:  0.9936132\n",
            "Training accuracy:  0.6628363\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  0.93744123\n",
            "Accuracy:  0.6717\n",
            "\n",
            "\n",
            "Epoch:  21\n",
            "Training data\n",
            "Training loss:  0.9750388\n",
            "Training accuracy:  0.6713845\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  0.9488436\n",
            "Accuracy:  0.6691\n",
            "\n",
            "\n",
            "Epoch:  22\n",
            "Training data\n",
            "Training loss:  0.96459585\n",
            "Training accuracy:  0.67658955\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  0.9664316\n",
            "Accuracy:  0.6652\n",
            "\n",
            "\n",
            "Epoch:  23\n",
            "Training data\n",
            "Training loss:  0.9311899\n",
            "Training accuracy:  0.6855181\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  0.9579978\n",
            "Accuracy:  0.6661\n",
            "\n",
            "\n",
            "Epoch:  24\n",
            "Training data\n",
            "Training loss:  0.91657734\n",
            "Training accuracy:  0.691684\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  0.9094584\n",
            "Accuracy:  0.6843\n",
            "\n",
            "\n",
            "Epoch:  25\n",
            "Training data\n",
            "Training loss:  0.8994584\n",
            "Training accuracy:  0.69891095\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  0.8722602\n",
            "Accuracy:  0.6979\n",
            "\n",
            "\n",
            "Epoch:  26\n",
            "Training data\n",
            "Training loss:  0.88290465\n",
            "Training accuracy:  0.70215404\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  0.8747258\n",
            "Accuracy:  0.6913\n",
            "\n",
            "\n",
            "Epoch:  27\n",
            "Training data\n",
            "Training loss:  0.8636457\n",
            "Training accuracy:  0.710382\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  0.8641191\n",
            "Accuracy:  0.7043\n",
            "\n",
            "\n",
            "Epoch:  28\n",
            "Training data\n",
            "Training loss:  0.8463356\n",
            "Training accuracy:  0.7171685\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  0.8729621\n",
            "Accuracy:  0.6939\n",
            "\n",
            "\n",
            "Epoch:  29\n",
            "Training data\n",
            "Training loss:  0.8292637\n",
            "Training accuracy:  0.7220732\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  0.89246434\n",
            "Accuracy:  0.6905\n",
            "\n",
            "\n",
            "Epoch:  30\n",
            "Training data\n",
            "Training loss:  0.8087875\n",
            "Training accuracy:  0.7277787\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  0.9193345\n",
            "Accuracy:  0.6808\n",
            "\n",
            "\n",
            "Epoch:  31\n",
            "Training data\n",
            "Training loss:  0.8012406\n",
            "Training accuracy:  0.73182255\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  0.8640322\n",
            "Accuracy:  0.702\n",
            "\n",
            "\n",
            "Epoch:  32\n",
            "Training data\n",
            "Training loss:  0.7867192\n",
            "Training accuracy:  0.7371477\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  0.8496149\n",
            "Accuracy:  0.7132\n",
            "\n",
            "\n",
            "Epoch:  33\n",
            "Training data\n",
            "Training loss:  0.77894723\n",
            "Training accuracy:  0.7399103\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  0.8840215\n",
            "Accuracy:  0.7016\n",
            "\n",
            "\n",
            "Epoch:  34\n",
            "Training data\n",
            "Training loss:  0.7720862\n",
            "Training accuracy:  0.74149185\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  0.8353839\n",
            "Accuracy:  0.7165\n",
            "\n",
            "\n",
            "Epoch:  35\n",
            "Training data\n",
            "Training loss:  0.7568841\n",
            "Training accuracy:  0.7476177\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  0.83369404\n",
            "Accuracy:  0.7188\n",
            "\n",
            "\n",
            "Epoch:  36\n",
            "Training data\n",
            "Training loss:  0.73986393\n",
            "Training accuracy:  0.75298285\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  0.82274175\n",
            "Accuracy:  0.7242\n",
            "\n",
            "\n",
            "Epoch:  37\n",
            "Training data\n",
            "Training loss:  0.7372595\n",
            "Training accuracy:  0.752082\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  0.8374173\n",
            "Accuracy:  0.7221\n",
            "\n",
            "\n",
            "Epoch:  38\n",
            "Training data\n",
            "Training loss:  0.73200375\n",
            "Training accuracy:  0.75932896\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  0.9126715\n",
            "Accuracy:  0.7017\n",
            "\n",
            "\n",
            "Epoch:  39\n",
            "Training data\n",
            "Training loss:  0.71726835\n",
            "Training accuracy:  0.7627122\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  0.8704646\n",
            "Accuracy:  0.7093\n",
            "\n",
            "\n",
            "Epoch:  40\n",
            "Training data\n",
            "Training loss:  0.70159423\n",
            "Training accuracy:  0.76411355\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  0.8413366\n",
            "Accuracy:  0.7211\n",
            "\n",
            "\n",
            "Epoch:  41\n",
            "Training data\n",
            "Training loss:  0.6931231\n",
            "Training accuracy:  0.7678972\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  0.85315794\n",
            "Accuracy:  0.723\n",
            "\n",
            "\n",
            "Epoch:  42\n",
            "Training data\n",
            "Training loss:  0.68136597\n",
            "Training accuracy:  0.773983\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  0.85798293\n",
            "Accuracy:  0.7171\n",
            "\n",
            "\n",
            "Epoch:  43\n",
            "Training data\n",
            "Training loss:  0.6736496\n",
            "Training accuracy:  0.7764454\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  0.8720261\n",
            "Accuracy:  0.717\n",
            "\n",
            "\n",
            "Epoch:  44\n",
            "Training data\n",
            "Training loss:  0.6634644\n",
            "Training accuracy:  0.77950835\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  0.8330089\n",
            "Accuracy:  0.7328\n",
            "\n",
            "\n",
            "Epoch:  45\n",
            "Training data\n",
            "Training loss:  0.6629282\n",
            "Training accuracy:  0.7824111\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  0.8468834\n",
            "Accuracy:  0.7254\n",
            "\n",
            "\n",
            "Epoch:  46\n",
            "Training data\n",
            "Training loss:  0.64525366\n",
            "Training accuracy:  0.78527385\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  0.7979735\n",
            "Accuracy:  0.7341\n",
            "\n",
            "\n",
            "Epoch:  47\n",
            "Training data\n",
            "Training loss:  0.6410033\n",
            "Training accuracy:  0.78433293\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  0.82986516\n",
            "Accuracy:  0.7347\n",
            "\n",
            "\n",
            "Epoch:  48\n",
            "Training data\n",
            "Training loss:  0.63440406\n",
            "Training accuracy:  0.7892577\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  0.86085373\n",
            "Accuracy:  0.7234\n",
            "\n",
            "\n",
            "Epoch:  49\n",
            "Training data\n",
            "Training loss:  0.63632774\n",
            "Training accuracy:  0.7891976\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  0.8882038\n",
            "Accuracy:  0.7227\n",
            "\n",
            "\n",
            "Epoch:  50\n",
            "Training data\n",
            "Training loss:  0.61870533\n",
            "Training accuracy:  0.79464287\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  0.85086155\n",
            "Accuracy:  0.7294\n",
            "\n",
            "\n",
            "Epoch:  51\n",
            "Training data\n",
            "Training loss:  0.61974835\n",
            "Training accuracy:  0.7932415\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  0.82574767\n",
            "Accuracy:  0.7341\n",
            "\n",
            "\n",
            "Epoch:  52\n",
            "Training data\n",
            "Training loss:  0.6080366\n",
            "Training accuracy:  0.7969851\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  0.82363534\n",
            "Accuracy:  0.734\n",
            "\n",
            "\n",
            "Epoch:  53\n",
            "Training data\n",
            "Training loss:  0.59146434\n",
            "Training accuracy:  0.8007287\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  0.8402228\n",
            "Accuracy:  0.7338\n",
            "\n",
            "\n",
            "Epoch:  54\n",
            "Training data\n",
            "Training loss:  0.60499924\n",
            "Training accuracy:  0.80028826\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  0.875474\n",
            "Accuracy:  0.7237\n",
            "\n",
            "\n",
            "Epoch:  55\n",
            "Training data\n",
            "Training loss:  0.5859988\n",
            "Training accuracy:  0.8049928\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  0.88056743\n",
            "Accuracy:  0.7262\n",
            "\n",
            "\n",
            "Epoch:  56\n",
            "Training data\n",
            "Training loss:  0.58800143\n",
            "Training accuracy:  0.80503285\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  0.86382407\n",
            "Accuracy:  0.7386\n",
            "\n",
            "\n",
            "Epoch:  57\n",
            "Training data\n",
            "Training loss:  0.58223873\n",
            "Training accuracy:  0.80809575\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  0.87197214\n",
            "Accuracy:  0.7317\n",
            "\n",
            "\n",
            "Epoch:  58\n",
            "Training data\n",
            "Training loss:  0.57692176\n",
            "Training accuracy:  0.8086763\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  0.8693242\n",
            "Accuracy:  0.7192\n",
            "\n",
            "\n",
            "Epoch:  59\n",
            "Training data\n",
            "Training loss:  0.56791013\n",
            "Training accuracy:  0.8106582\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  0.85204065\n",
            "Accuracy:  0.7329\n",
            "\n",
            "\n",
            "Epoch:  60\n",
            "Training data\n",
            "Training loss:  0.561094\n",
            "Training accuracy:  0.8141015\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  0.8778941\n",
            "Accuracy:  0.7397\n",
            "\n",
            "\n",
            "Epoch:  61\n",
            "Training data\n",
            "Training loss:  0.5567589\n",
            "Training accuracy:  0.8160434\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  0.8621237\n",
            "Accuracy:  0.7344\n",
            "\n",
            "\n",
            "Epoch:  62\n",
            "Training data\n",
            "Training loss:  0.548172\n",
            "Training accuracy:  0.81694424\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  0.86905336\n",
            "Accuracy:  0.7352\n",
            "\n",
            "\n",
            "Epoch:  63\n",
            "Training data\n",
            "Training loss:  0.55498284\n",
            "Training accuracy:  0.8139814\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  0.89797837\n",
            "Accuracy:  0.7244\n",
            "\n",
            "\n",
            "Epoch:  64\n",
            "Training data\n",
            "Training loss:  0.55805135\n",
            "Training accuracy:  0.8115791\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  0.89751345\n",
            "Accuracy:  0.7359\n",
            "\n",
            "\n",
            "Epoch:  65\n",
            "Training data\n",
            "Training loss:  0.54906464\n",
            "Training accuracy:  0.8216488\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  0.87771726\n",
            "Accuracy:  0.7434\n",
            "\n",
            "\n",
            "Epoch:  66\n",
            "Training data\n",
            "Training loss:  0.54013455\n",
            "Training accuracy:  0.8222694\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  0.88095045\n",
            "Accuracy:  0.7342\n",
            "\n",
            "\n",
            "Epoch:  67\n",
            "Training data\n",
            "Training loss:  0.5482946\n",
            "Training accuracy:  0.81898624\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  0.8918984\n",
            "Accuracy:  0.7314\n",
            "\n",
            "\n",
            "Epoch:  68\n",
            "Training data\n",
            "Training loss:  0.5345238\n",
            "Training accuracy:  0.8251922\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  0.87170887\n",
            "Accuracy:  0.7431\n",
            "\n",
            "\n",
            "Epoch:  69\n",
            "Training data\n",
            "Training loss:  0.5341879\n",
            "Training accuracy:  0.82385087\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  0.9701324\n",
            "Accuracy:  0.715\n",
            "\n",
            "\n",
            "Epoch:  70\n",
            "Training data\n",
            "Training loss:  0.5170727\n",
            "Training accuracy:  0.8301569\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  0.86981016\n",
            "Accuracy:  0.7381\n",
            "\n",
            "\n",
            "Epoch:  71\n",
            "Training data\n",
            "Training loss:  0.51558304\n",
            "Training accuracy:  0.8281951\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  0.9327428\n",
            "Accuracy:  0.7375\n",
            "\n",
            "\n",
            "Epoch:  72\n",
            "Training data\n",
            "Training loss:  0.5085806\n",
            "Training accuracy:  0.83061737\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  0.8852909\n",
            "Accuracy:  0.7394\n",
            "\n",
            "\n",
            "Epoch:  73\n",
            "Training data\n",
            "Training loss:  0.5079516\n",
            "Training accuracy:  0.8306574\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  0.8768159\n",
            "Accuracy:  0.7444\n",
            "\n",
            "\n",
            "Epoch:  74\n",
            "Training data\n",
            "Training loss:  0.49877527\n",
            "Training accuracy:  0.8374039\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  0.89456546\n",
            "Accuracy:  0.746\n",
            "\n",
            "\n",
            "Epoch:  75\n",
            "Training data\n",
            "Training loss:  0.51587576\n",
            "Training accuracy:  0.83043724\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  0.8330391\n",
            "Accuracy:  0.7475\n",
            "\n",
            "\n",
            "Epoch:  76\n",
            "Training data\n",
            "Training loss:  0.50553834\n",
            "Training accuracy:  0.83253926\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  0.91916925\n",
            "Accuracy:  0.7328\n",
            "\n",
            "\n",
            "Epoch:  77\n",
            "Training data\n",
            "Training loss:  0.49640337\n",
            "Training accuracy:  0.83590245\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  0.93135846\n",
            "Accuracy:  0.7382\n",
            "\n",
            "\n",
            "Epoch:  78\n",
            "Training data\n",
            "Training loss:  0.4995861\n",
            "Training accuracy:  0.83596253\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  0.9512377\n",
            "Accuracy:  0.733\n",
            "\n",
            "\n",
            "Epoch:  79\n",
            "Training data\n",
            "Training loss:  0.4836199\n",
            "Training accuracy:  0.8404268\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  0.89720124\n",
            "Accuracy:  0.7466\n",
            "\n",
            "\n",
            "Epoch:  80\n",
            "Training data\n",
            "Training loss:  0.47723523\n",
            "Training accuracy:  0.83960605\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  0.90007144\n",
            "Accuracy:  0.7338\n",
            "\n",
            "\n",
            "Epoch:  81\n",
            "Training data\n",
            "Training loss:  0.48187864\n",
            "Training accuracy:  0.8404869\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  0.9072398\n",
            "Accuracy:  0.7407\n",
            "\n",
            "\n",
            "Epoch:  82\n",
            "Training data\n",
            "Training loss:  0.478698\n",
            "Training accuracy:  0.8419683\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  0.91399604\n",
            "Accuracy:  0.7396\n",
            "\n",
            "\n",
            "Epoch:  83\n",
            "Training data\n",
            "Training loss:  0.47825545\n",
            "Training accuracy:  0.84222853\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  0.8764555\n",
            "Accuracy:  0.7487\n",
            "\n",
            "\n",
            "Epoch:  84\n",
            "Training data\n",
            "Training loss:  0.471559\n",
            "Training accuracy:  0.8428892\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  0.8766903\n",
            "Accuracy:  0.7454\n",
            "\n",
            "\n",
            "Epoch:  85\n",
            "Training data\n",
            "Training loss:  0.46838313\n",
            "Training accuracy:  0.8450312\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  0.86618847\n",
            "Accuracy:  0.7415\n",
            "\n",
            "\n",
            "Epoch:  86\n",
            "Training data\n",
            "Training loss:  0.47390372\n",
            "Training accuracy:  0.8435098\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  0.9690369\n",
            "Accuracy:  0.73\n",
            "\n",
            "\n",
            "Epoch:  87\n",
            "Training data\n",
            "Training loss:  0.46619496\n",
            "Training accuracy:  0.8478139\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  0.91657895\n",
            "Accuracy:  0.7483\n",
            "\n",
            "\n",
            "Epoch:  88\n",
            "Training data\n",
            "Training loss:  0.4667632\n",
            "Training accuracy:  0.846873\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  0.9132855\n",
            "Accuracy:  0.7546\n",
            "\n",
            "\n",
            "Epoch:  89\n",
            "Training data\n",
            "Training loss:  0.46650144\n",
            "Training accuracy:  0.84649265\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  0.870398\n",
            "Accuracy:  0.7497\n",
            "\n",
            "\n",
            "Epoch:  90\n",
            "Training data\n",
            "Training loss:  0.46354878\n",
            "Training accuracy:  0.8486747\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  0.9554838\n",
            "Accuracy:  0.7392\n",
            "\n",
            "\n",
            "Epoch:  91\n",
            "Training data\n",
            "Training loss:  0.4482081\n",
            "Training accuracy:  0.8494154\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  0.9034504\n",
            "Accuracy:  0.7431\n",
            "\n",
            "\n",
            "Epoch:  92\n",
            "Training data\n",
            "Training loss:  0.45259994\n",
            "Training accuracy:  0.8509569\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  0.9429281\n",
            "Accuracy:  0.7488\n",
            "\n",
            "\n",
            "Epoch:  93\n",
            "Training data\n",
            "Training loss:  0.4547797\n",
            "Training accuracy:  0.8511171\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  0.90284204\n",
            "Accuracy:  0.7416\n",
            "\n",
            "\n",
            "Epoch:  94\n",
            "Training data\n",
            "Training loss:  0.45084414\n",
            "Training accuracy:  0.852118\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  0.95142275\n",
            "Accuracy:  0.7462\n",
            "\n",
            "\n",
            "Epoch:  95\n",
            "Training data\n",
            "Training loss:  0.44579414\n",
            "Training accuracy:  0.8546405\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  0.915447\n",
            "Accuracy:  0.7411\n",
            "\n",
            "\n",
            "Epoch:  96\n",
            "Training data\n",
            "Training loss:  0.4434444\n",
            "Training accuracy:  0.85416\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  0.90566355\n",
            "Accuracy:  0.7455\n",
            "\n",
            "\n",
            "Epoch:  97\n",
            "Training data\n",
            "Training loss:  0.43933025\n",
            "Training accuracy:  0.8569627\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  0.91546434\n",
            "Accuracy:  0.75\n",
            "\n",
            "\n",
            "Epoch:  98\n",
            "Training data\n",
            "Training loss:  0.4405876\n",
            "Training accuracy:  0.85546124\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  0.9274668\n",
            "Accuracy:  0.7502\n",
            "\n",
            "\n",
            "Epoch:  99\n",
            "Training data\n",
            "Training loss:  0.44501057\n",
            "Training accuracy:  0.8533993\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  0.9088023\n",
            "Accuracy:  0.7485\n",
            "\n",
            "\n",
            "Epoch:  100\n",
            "Training data\n",
            "Training loss:  0.4422229\n",
            "Training accuracy:  0.8561619\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  0.9239716\n",
            "Accuracy:  0.7534\n",
            "\n",
            "\n",
            "Epoch:  101\n",
            "Training data\n",
            "Training loss:  0.4412921\n",
            "Training accuracy:  0.8553812\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  0.9228056\n",
            "Accuracy:  0.7463\n",
            "\n",
            "\n",
            "Epoch:  102\n",
            "Training data\n",
            "Training loss:  0.43829325\n",
            "Training accuracy:  0.8580237\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  0.89308554\n",
            "Accuracy:  0.7541\n",
            "\n",
            "\n",
            "Epoch:  103\n",
            "Training data\n",
            "Training loss:  0.43665922\n",
            "Training accuracy:  0.85824394\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  0.9754932\n",
            "Accuracy:  0.7386\n",
            "\n",
            "\n",
            "Epoch:  104\n",
            "Training data\n",
            "Training loss:  0.42613435\n",
            "Training accuracy:  0.8588245\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  1.0060909\n",
            "Accuracy:  0.7361\n",
            "\n",
            "\n",
            "Epoch:  105\n",
            "Training data\n",
            "Training loss:  0.42291287\n",
            "Training accuracy:  0.8605461\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  0.9973257\n",
            "Accuracy:  0.7414\n",
            "\n",
            "\n",
            "Epoch:  106\n",
            "Training data\n",
            "Training loss:  0.42385322\n",
            "Training accuracy:  0.86118674\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  0.90059745\n",
            "Accuracy:  0.7496\n",
            "\n",
            "\n",
            "Epoch:  107\n",
            "Training data\n",
            "Training loss:  0.42794448\n",
            "Training accuracy:  0.8595652\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  0.90237415\n",
            "Accuracy:  0.7466\n",
            "\n",
            "\n",
            "Epoch:  108\n",
            "Training data\n",
            "Training loss:  0.42054287\n",
            "Training accuracy:  0.8631486\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  0.9417634\n",
            "Accuracy:  0.7442\n",
            "\n",
            "\n",
            "Epoch:  109\n",
            "Training data\n",
            "Training loss:  0.4300169\n",
            "Training accuracy:  0.8599255\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  0.93046\n",
            "Accuracy:  0.745\n",
            "\n",
            "\n",
            "Epoch:  110\n",
            "Training data\n",
            "Training loss:  0.4173033\n",
            "Training accuracy:  0.86382926\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  0.8993331\n",
            "Accuracy:  0.7489\n",
            "\n",
            "\n",
            "Epoch:  111\n",
            "Training data\n",
            "Training loss:  0.41852924\n",
            "Training accuracy:  0.8648102\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  1.0230716\n",
            "Accuracy:  0.7414\n",
            "\n",
            "\n",
            "Epoch:  112\n",
            "Training data\n",
            "Training loss:  0.41753352\n",
            "Training accuracy:  0.8637492\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  1.0229996\n",
            "Accuracy:  0.7397\n",
            "\n",
            "\n",
            "Epoch:  113\n",
            "Training data\n",
            "Training loss:  0.41399682\n",
            "Training accuracy:  0.8637292\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  0.9462077\n",
            "Accuracy:  0.7361\n",
            "\n",
            "\n",
            "Epoch:  114\n",
            "Training data\n",
            "Training loss:  0.4127403\n",
            "Training accuracy:  0.8647301\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  0.9413112\n",
            "Accuracy:  0.7454\n",
            "\n",
            "\n",
            "Epoch:  115\n",
            "Training data\n",
            "Training loss:  0.40940323\n",
            "Training accuracy:  0.8649704\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  0.9996407\n",
            "Accuracy:  0.7349\n",
            "\n",
            "\n",
            "Epoch:  116\n",
            "Training data\n",
            "Training loss:  0.409433\n",
            "Training accuracy:  0.86621153\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  0.92017555\n",
            "Accuracy:  0.7458\n",
            "\n",
            "\n",
            "Epoch:  117\n",
            "Training data\n",
            "Training loss:  0.41175768\n",
            "Training accuracy:  0.86463004\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  0.94283193\n",
            "Accuracy:  0.741\n",
            "\n",
            "\n",
            "Epoch:  118\n",
            "Training data\n",
            "Training loss:  0.40443814\n",
            "Training accuracy:  0.86661196\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  0.9944076\n",
            "Accuracy:  0.7441\n",
            "\n",
            "\n",
            "Epoch:  119\n",
            "Training data\n",
            "Training loss:  0.40413398\n",
            "Training accuracy:  0.8674928\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  1.0535324\n",
            "Accuracy:  0.7352\n",
            "\n",
            "\n",
            "Epoch:  120\n",
            "Training data\n",
            "Training loss:  0.41166246\n",
            "Training accuracy:  0.86611146\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  0.973176\n",
            "Accuracy:  0.748\n",
            "\n",
            "\n",
            "Epoch:  121\n",
            "Training data\n",
            "Training loss:  0.40257803\n",
            "Training accuracy:  0.86795324\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  0.96033317\n",
            "Accuracy:  0.746\n",
            "\n",
            "\n",
            "Epoch:  122\n",
            "Training data\n",
            "Training loss:  0.39912122\n",
            "Training accuracy:  0.86911434\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  0.9550885\n",
            "Accuracy:  0.7456\n",
            "\n",
            "\n",
            "Epoch:  123\n",
            "Training data\n",
            "Training loss:  0.3976802\n",
            "Training accuracy:  0.87029546\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  0.9518616\n",
            "Accuracy:  0.7479\n",
            "\n",
            "\n",
            "Epoch:  124\n",
            "Training data\n",
            "Training loss:  0.39810356\n",
            "Training accuracy:  0.8695748\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  0.9679959\n",
            "Accuracy:  0.7515\n",
            "\n",
            "\n",
            "Epoch:  125\n",
            "Training data\n",
            "Training loss:  0.3935198\n",
            "Training accuracy:  0.8707559\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  1.0744528\n",
            "Accuracy:  0.7502\n",
            "\n",
            "\n",
            "Epoch:  126\n",
            "Training data\n",
            "Training loss:  0.38697726\n",
            "Training accuracy:  0.87504005\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  0.9606401\n",
            "Accuracy:  0.7456\n",
            "\n",
            "\n",
            "Epoch:  127\n",
            "Training data\n",
            "Training loss:  0.395566\n",
            "Training accuracy:  0.8696949\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  1.047627\n",
            "Accuracy:  0.7393\n",
            "\n",
            "\n",
            "Epoch:  128\n",
            "Training data\n",
            "Training loss:  0.39090204\n",
            "Training accuracy:  0.872918\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  0.9798787\n",
            "Accuracy:  0.7446\n",
            "\n",
            "\n",
            "Epoch:  129\n",
            "Training data\n",
            "Training loss:  0.39297497\n",
            "Training accuracy:  0.87089604\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  1.011167\n",
            "Accuracy:  0.7448\n",
            "\n",
            "\n",
            "Epoch:  130\n",
            "Training data\n",
            "Training loss:  0.38871703\n",
            "Training accuracy:  0.8733184\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  0.94346803\n",
            "Accuracy:  0.7412\n",
            "\n",
            "\n",
            "Epoch:  131\n",
            "Training data\n",
            "Training loss:  0.3771182\n",
            "Training accuracy:  0.87806296\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  0.9794086\n",
            "Accuracy:  0.7471\n",
            "\n",
            "\n",
            "Epoch:  132\n",
            "Training data\n",
            "Training loss:  0.38856605\n",
            "Training accuracy:  0.873939\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  1.0155754\n",
            "Accuracy:  0.7409\n",
            "\n",
            "\n",
            "Epoch:  133\n",
            "Training data\n",
            "Training loss:  0.39349514\n",
            "Training accuracy:  0.87127644\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  1.0063348\n",
            "Accuracy:  0.7493\n",
            "\n",
            "\n",
            "Epoch:  134\n",
            "Training data\n",
            "Training loss:  0.3789914\n",
            "Training accuracy:  0.87576073\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  1.0260404\n",
            "Accuracy:  0.7439\n",
            "\n",
            "\n",
            "Epoch:  135\n",
            "Training data\n",
            "Training loss:  0.37814796\n",
            "Training accuracy:  0.87910396\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  0.97479343\n",
            "Accuracy:  0.7483\n",
            "\n",
            "\n",
            "Epoch:  136\n",
            "Training data\n",
            "Training loss:  0.37815264\n",
            "Training accuracy:  0.87652147\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  1.0959396\n",
            "Accuracy:  0.7411\n",
            "\n",
            "\n",
            "Epoch:  137\n",
            "Training data\n",
            "Training loss:  0.38330168\n",
            "Training accuracy:  0.8743194\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  0.9897766\n",
            "Accuracy:  0.7522\n",
            "\n",
            "\n",
            "Epoch:  138\n",
            "Training data\n",
            "Training loss:  0.3799678\n",
            "Training accuracy:  0.8758608\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  1.094183\n",
            "Accuracy:  0.7409\n",
            "\n",
            "\n",
            "Epoch:  139\n",
            "Training data\n",
            "Training loss:  0.372964\n",
            "Training accuracy:  0.87818307\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  1.0417907\n",
            "Accuracy:  0.7456\n",
            "\n",
            "\n",
            "Epoch:  140\n",
            "Training data\n",
            "Training loss:  0.3763235\n",
            "Training accuracy:  0.8774023\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  1.0317792\n",
            "Accuracy:  0.7412\n",
            "\n",
            "\n",
            "Epoch:  141\n",
            "Training data\n",
            "Training loss:  0.37302864\n",
            "Training accuracy:  0.8804252\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  1.0576077\n",
            "Accuracy:  0.746\n",
            "\n",
            "\n",
            "Epoch:  142\n",
            "Training data\n",
            "Training loss:  0.38566026\n",
            "Training accuracy:  0.87461966\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  1.0067827\n",
            "Accuracy:  0.7403\n",
            "\n",
            "\n",
            "Epoch:  143\n",
            "Training data\n",
            "Training loss:  0.3676272\n",
            "Training accuracy:  0.8809057\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  0.95962614\n",
            "Accuracy:  0.7496\n",
            "\n",
            "\n",
            "Epoch:  144\n",
            "Training data\n",
            "Training loss:  0.37362742\n",
            "Training accuracy:  0.878143\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  1.1466422\n",
            "Accuracy:  0.7303\n",
            "\n",
            "\n",
            "Epoch:  145\n",
            "Training data\n",
            "Training loss:  0.37181482\n",
            "Training accuracy:  0.87836325\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  0.98565084\n",
            "Accuracy:  0.7419\n",
            "\n",
            "\n",
            "Epoch:  146\n",
            "Training data\n",
            "Training loss:  0.36869103\n",
            "Training accuracy:  0.88234705\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  1.1532626\n",
            "Accuracy:  0.7448\n",
            "\n",
            "\n",
            "Epoch:  147\n",
            "Training data\n",
            "Training loss:  0.37603813\n",
            "Training accuracy:  0.879164\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  1.1014642\n",
            "Accuracy:  0.7423\n",
            "\n",
            "\n",
            "Epoch:  148\n",
            "Training data\n",
            "Training loss:  0.3696749\n",
            "Training accuracy:  0.8796244\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  1.0720029\n",
            "Accuracy:  0.7399\n",
            "\n",
            "\n",
            "Epoch:  149\n",
            "Training data\n",
            "Training loss:  0.3669108\n",
            "Training accuracy:  0.8832479\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  0.9967213\n",
            "Accuracy:  0.7479\n",
            "\n",
            "\n",
            "Epoch:  150\n",
            "Training data\n",
            "Training loss:  0.3707181\n",
            "Training accuracy:  0.8807855\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  0.99991494\n",
            "Accuracy:  0.7416\n",
            "\n",
            "\n",
            "Epoch:  151\n",
            "Training data\n",
            "Training loss:  0.36133265\n",
            "Training accuracy:  0.8826874\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  1.0775138\n",
            "Accuracy:  0.7445\n",
            "\n",
            "\n",
            "Epoch:  152\n",
            "Training data\n",
            "Training loss:  0.36112413\n",
            "Training accuracy:  0.88212687\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  1.0537014\n",
            "Accuracy:  0.7511\n",
            "\n",
            "\n",
            "Epoch:  153\n",
            "Training data\n",
            "Training loss:  0.37399948\n",
            "Training accuracy:  0.8785434\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  1.1068859\n",
            "Accuracy:  0.7422\n",
            "\n",
            "\n",
            "Epoch:  154\n",
            "Training data\n",
            "Training loss:  0.3643087\n",
            "Training accuracy:  0.88264734\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  1.1166037\n",
            "Accuracy:  0.7453\n",
            "\n",
            "\n",
            "Epoch:  155\n",
            "Training data\n",
            "Training loss:  0.3649823\n",
            "Training accuracy:  0.88080555\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  1.041425\n",
            "Accuracy:  0.7502\n",
            "\n",
            "\n",
            "Epoch:  156\n",
            "Training data\n",
            "Training loss:  0.3495138\n",
            "Training accuracy:  0.88765216\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  1.0374632\n",
            "Accuracy:  0.7508\n",
            "\n",
            "\n",
            "Epoch:  157\n",
            "Training data\n",
            "Training loss:  0.35774\n",
            "Training accuracy:  0.88412875\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  1.0584246\n",
            "Accuracy:  0.7397\n",
            "\n",
            "\n",
            "Epoch:  158\n",
            "Training data\n",
            "Training loss:  0.35793737\n",
            "Training accuracy:  0.883368\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  0.9987739\n",
            "Accuracy:  0.7457\n",
            "\n",
            "\n",
            "Epoch:  159\n",
            "Training data\n",
            "Training loss:  0.3670405\n",
            "Training accuracy:  0.88106585\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  1.0778202\n",
            "Accuracy:  0.7428\n",
            "\n",
            "\n",
            "Epoch:  160\n",
            "Training data\n",
            "Training loss:  0.35463217\n",
            "Training accuracy:  0.88547003\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  1.0228044\n",
            "Accuracy:  0.7524\n",
            "\n",
            "\n",
            "Epoch:  161\n",
            "Training data\n",
            "Training loss:  0.35501334\n",
            "Training accuracy:  0.8855301\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  1.0759932\n",
            "Accuracy:  0.7395\n",
            "\n",
            "\n",
            "Epoch:  162\n",
            "Training data\n",
            "Training loss:  0.35955897\n",
            "Training accuracy:  0.8848895\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  1.0902767\n",
            "Accuracy:  0.744\n",
            "\n",
            "\n",
            "Epoch:  163\n",
            "Training data\n",
            "Training loss:  0.3486117\n",
            "Training accuracy:  0.8863309\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  1.036204\n",
            "Accuracy:  0.7513\n",
            "\n",
            "\n",
            "Epoch:  164\n",
            "Training data\n",
            "Training loss:  0.35318223\n",
            "Training accuracy:  0.8856903\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  1.0409722\n",
            "Accuracy:  0.7428\n",
            "\n",
            "\n",
            "Epoch:  165\n",
            "Training data\n",
            "Training loss:  0.3456581\n",
            "Training accuracy:  0.88857305\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  1.0201039\n",
            "Accuracy:  0.7465\n",
            "\n",
            "\n",
            "Epoch:  166\n",
            "Training data\n",
            "Training loss:  0.34704566\n",
            "Training accuracy:  0.88725173\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  1.0747936\n",
            "Accuracy:  0.7472\n",
            "\n",
            "\n",
            "Epoch:  167\n",
            "Training data\n",
            "Training loss:  0.3495031\n",
            "Training accuracy:  0.886431\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  1.0762174\n",
            "Accuracy:  0.748\n",
            "\n",
            "\n",
            "Epoch:  168\n",
            "Training data\n",
            "Training loss:  0.35098186\n",
            "Training accuracy:  0.88617074\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  1.1125696\n",
            "Accuracy:  0.7436\n",
            "\n",
            "\n",
            "Epoch:  169\n",
            "Training data\n",
            "Training loss:  0.35031074\n",
            "Training accuracy:  0.8871917\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  1.0308707\n",
            "Accuracy:  0.7517\n",
            "\n",
            "\n",
            "Epoch:  170\n",
            "Training data\n",
            "Training loss:  0.34613708\n",
            "Training accuracy:  0.8878123\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  1.1149837\n",
            "Accuracy:  0.7424\n",
            "\n",
            "\n",
            "Epoch:  171\n",
            "Training data\n",
            "Training loss:  0.34751692\n",
            "Training accuracy:  0.88691145\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  1.1251497\n",
            "Accuracy:  0.7457\n",
            "\n",
            "\n",
            "Epoch:  172\n",
            "Training data\n",
            "Training loss:  0.34698072\n",
            "Training accuracy:  0.8897742\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  1.1277999\n",
            "Accuracy:  0.7435\n",
            "\n",
            "\n",
            "Epoch:  173\n",
            "Training data\n",
            "Training loss:  0.34571397\n",
            "Training accuracy:  0.88827276\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  1.1318952\n",
            "Accuracy:  0.7464\n",
            "\n",
            "\n",
            "Epoch:  174\n",
            "Training data\n",
            "Training loss:  0.34535685\n",
            "Training accuracy:  0.88775223\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  1.0897508\n",
            "Accuracy:  0.7487\n",
            "\n",
            "\n",
            "Epoch:  175\n",
            "Training data\n",
            "Training loss:  0.34586954\n",
            "Training accuracy:  0.8876722\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  1.121263\n",
            "Accuracy:  0.744\n",
            "\n",
            "\n",
            "Epoch:  176\n",
            "Training data\n",
            "Training loss:  0.34740654\n",
            "Training accuracy:  0.8883528\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  1.1027551\n",
            "Accuracy:  0.7458\n",
            "\n",
            "\n",
            "Epoch:  177\n",
            "Training data\n",
            "Training loss:  0.34120178\n",
            "Training accuracy:  0.89003444\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  1.0125471\n",
            "Accuracy:  0.7514\n",
            "\n",
            "\n",
            "Epoch:  178\n",
            "Training data\n",
            "Training loss:  0.34785956\n",
            "Training accuracy:  0.88955396\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  1.0780104\n",
            "Accuracy:  0.7476\n",
            "\n",
            "\n",
            "Epoch:  179\n",
            "Training data\n",
            "Training loss:  0.33763513\n",
            "Training accuracy:  0.89097536\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  1.0575912\n",
            "Accuracy:  0.746\n",
            "\n",
            "\n",
            "Epoch:  180\n",
            "Training data\n",
            "Training loss:  0.3397729\n",
            "Training accuracy:  0.89139575\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  1.100709\n",
            "Accuracy:  0.7512\n",
            "\n",
            "\n",
            "Epoch:  181\n",
            "Training data\n",
            "Training loss:  0.3383707\n",
            "Training accuracy:  0.8904548\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  1.0956547\n",
            "Accuracy:  0.746\n",
            "\n",
            "\n",
            "Epoch:  182\n",
            "Training data\n",
            "Training loss:  0.3400241\n",
            "Training accuracy:  0.8917761\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  1.1219747\n",
            "Accuracy:  0.7478\n",
            "\n",
            "\n",
            "Epoch:  183\n",
            "Training data\n",
            "Training loss:  0.33844674\n",
            "Training accuracy:  0.89101535\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  1.0980223\n",
            "Accuracy:  0.7492\n",
            "\n",
            "\n",
            "Epoch:  184\n",
            "Training data\n",
            "Training loss:  0.33519024\n",
            "Training accuracy:  0.89287716\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  0.9957487\n",
            "Accuracy:  0.7566\n",
            "\n",
            "\n",
            "Epoch:  185\n",
            "Training data\n",
            "Training loss:  0.32317197\n",
            "Training accuracy:  0.89577997\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  1.0280955\n",
            "Accuracy:  0.7525\n",
            "\n",
            "\n",
            "Epoch:  186\n",
            "Training data\n",
            "Training loss:  0.32886952\n",
            "Training accuracy:  0.893698\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  1.0323462\n",
            "Accuracy:  0.7567\n",
            "\n",
            "\n",
            "Epoch:  187\n",
            "Training data\n",
            "Training loss:  0.33065277\n",
            "Training accuracy:  0.8942785\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  1.09658\n",
            "Accuracy:  0.7491\n",
            "\n",
            "\n",
            "Epoch:  188\n",
            "Training data\n",
            "Training loss:  0.33893603\n",
            "Training accuracy:  0.89201635\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  1.0392789\n",
            "Accuracy:  0.7384\n",
            "\n",
            "\n",
            "Epoch:  189\n",
            "Training data\n",
            "Training loss:  0.32960808\n",
            "Training accuracy:  0.89213645\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  1.0280339\n",
            "Accuracy:  0.7505\n",
            "\n",
            "\n",
            "Epoch:  190\n",
            "Training data\n",
            "Training loss:  0.32967004\n",
            "Training accuracy:  0.8943185\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  1.169196\n",
            "Accuracy:  0.7451\n",
            "\n",
            "\n",
            "Epoch:  191\n",
            "Training data\n",
            "Training loss:  0.33699563\n",
            "Training accuracy:  0.89371794\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  1.0611501\n",
            "Accuracy:  0.748\n",
            "\n",
            "\n",
            "Epoch:  192\n",
            "Training data\n",
            "Training loss:  0.3339255\n",
            "Training accuracy:  0.89335763\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  1.0543833\n",
            "Accuracy:  0.7344\n",
            "\n",
            "\n",
            "Epoch:  193\n",
            "Training data\n",
            "Training loss:  0.33342978\n",
            "Training accuracy:  0.8919162\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  1.1605127\n",
            "Accuracy:  0.7384\n",
            "\n",
            "\n",
            "Epoch:  194\n",
            "Training data\n",
            "Training loss:  0.32605407\n",
            "Training accuracy:  0.8960602\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  1.0304832\n",
            "Accuracy:  0.753\n",
            "\n",
            "\n",
            "Epoch:  195\n",
            "Training data\n",
            "Training loss:  0.31889102\n",
            "Training accuracy:  0.8955597\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  1.0553762\n",
            "Accuracy:  0.7494\n",
            "\n",
            "\n",
            "Epoch:  196\n",
            "Training data\n",
            "Training loss:  0.3400649\n",
            "Training accuracy:  0.89047486\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  1.1040889\n",
            "Accuracy:  0.7519\n",
            "\n",
            "\n",
            "Epoch:  197\n",
            "Training data\n",
            "Training loss:  0.32833636\n",
            "Training accuracy:  0.8945788\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  1.1203545\n",
            "Accuracy:  0.7407\n",
            "\n",
            "\n",
            "Epoch:  198\n",
            "Training data\n",
            "Training loss:  0.32561156\n",
            "Training accuracy:  0.89680094\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  1.1332635\n",
            "Accuracy:  0.7469\n",
            "\n",
            "\n",
            "Epoch:  199\n",
            "Training data\n",
            "Training loss:  0.3186129\n",
            "Training accuracy:  0.8967609\n",
            "\n",
            "\n",
            "Test data\n",
            "Loss:  1.0787492\n",
            "Accuracy:  0.7417\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "YFJu_N0QfxZ0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Ignore what is below, only notes to self and trying stuff while debugging"
      ]
    },
    {
      "metadata": {
        "id": "IiQ0ak3PjOEO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# will stop the session here so that we can start a new session with dataset shuffling later\n",
        "\n",
        "# sess.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rQa3e8NM3zwH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9382af41-3a4c-4772-ae76-8e76f546ff25"
      },
      "cell_type": "code",
      "source": [
        "# need to debug why the loss keeps increasing - loss is over test set, did we overfit training set by not shuffling?\n",
        "\n",
        "'''The model has overfitted to the training data, perhaps we will shuffle the order of the data at every epoch'''"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The model has overfitted to the training data, perhaps we will shuffle the order of the data at every epoch'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "metadata": {
        "id": "TUm-zYEmp1H_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now we will shuffle the dataset by calling the built in function\n",
        "\n",
        "Need to go in this order, dataset.shuffle(no seed here).batch.repeat() to get 1 shuffle for 1 epoch, then batched, then next. repeat before shuffe will result in some appearing more than once in one epoch and some none at all.\n",
        "\n",
        "If batch comes before the shuffle, then the elements within the batch is in the original order, only the batch order is shuffled."
      ]
    },
    {
      "metadata": {
        "id": "5NNCeh0zpylL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "dataset_training = tf.data.Dataset.from_tensor_slices((x_train, y_train_one_hot)).shuffle(1000).batch(batch_size).repeat()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bFcuw9sZpylN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# going to create one more set of training dataset that is not batched but for testing training accuracy\n",
        "dataset_training_non_batch = tf.data.Dataset.from_tensor_slices((x_train, y_train_one_hot)).shuffle(1000).batch(num_training_instance).repeat()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZfRYJTrEpylQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "dataset_test = tf.data.Dataset.from_tensor_slices((x_test, y_test_one_hot)).shuffle(200).batch(num_test_instance).repeat()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "K37H1XOnpylR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "iterator = tf.data.Iterator.from_structure(dataset_training.output_types, dataset_training.output_shapes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1ThuaG-npylU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_init = iterator.make_initializer(dataset_training)\n",
        "test_init = iterator.make_initializer(dataset_test)\n",
        "\n",
        "train_whole_init = iterator.make_initializer(dataset_training_non_batch)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RdLGwGXQpylW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "image_batch, label_batch = iterator.get_next()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "87ZYqYYM0XvP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# sess = tf.Session()\n",
        "sess.run(tf.global_variables_initializer())\n",
        "sess.run(tf.local_variables_initializer())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GP04gYis0XvS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2428
        },
        "outputId": "a495ffe8-c314-470d-c701-c933e416f36d"
      },
      "cell_type": "code",
      "source": [
        "# training with evaluation after every epoch\n",
        "\n",
        "num_epoch = 10\n",
        "\n",
        "for i in range(num_epoch):\n",
        "  # print the current number of epoch\n",
        "  print('Epoch: ', i)\n",
        "  \n",
        "  # initialize the training dataset\n",
        "  sess.run(train_init)\n",
        "  \n",
        "  # run the training iterations for 1 epoch\n",
        "  for _ in range(num_train_iteration_per_epoch):\n",
        "    sess.run(train)\n",
        "    \n",
        "  # initialize the test dataset\n",
        "  sess.run(test_init)\n",
        "  \n",
        "  \n",
        "  print('Test data')\n",
        "  # evaluate the current training progress over the test data and print them\n",
        "  ## take in input data\n",
        "#   sess.run([iterator.get_next()])\n",
        "  \n",
        "#   sess.run(tf.local_variables_initializer())\n",
        "  \n",
        "  print('Loss: ', sess.run(cost))\n",
        "  \n",
        "  print('Accuracy: ', sess.run(accuracy))\n",
        "  \n",
        "  ####### going to test the model on training to understand the reduction in accuracy as training proceeds\n",
        "  \n",
        "#   num_correct = 0\n",
        "  \n",
        "#   print('Training data')\n",
        "  \n",
        "#   sess.run(train_init)\n",
        "#   for _ in range(num_train_iteration_per_epoch):\n",
        "    \n",
        "#   print('Training loss: ', sess.run(cost))\n",
        "#   print('Training accuracy: ', sess.run(accuracy))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch:  0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ResourceExhaustedError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1307\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m           run_metadata)\n\u001b[0m\u001b[1;32m   1410\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[50000,32,16,16] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[Node: conv_layer_2/Conv2D = Conv2D[T=DT_FLOAT, data_format=\"NCHW\", dilations=[1, 1, 1, 1], padding=\"SAME\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](batch_norm_1/FusedBatchNorm, conv_layer_2/kernel/read)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-4098f4ed3f77>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m   \u001b[0;31m# run the training iterations for 1 epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_train_iteration_per_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m   \u001b[0;31m# initialize the test dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 900\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    901\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1135\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1136\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1316\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1317\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1333\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1334\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1335\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1337\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[50000,32,16,16] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[Node: conv_layer_2/Conv2D = Conv2D[T=DT_FLOAT, data_format=\"NCHW\", dilations=[1, 1, 1, 1], padding=\"SAME\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](batch_norm_1/FusedBatchNorm, conv_layer_2/kernel/read)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\nCaused by op 'conv_layer_2/Conv2D', defined at:\n  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python3.6/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-19-99b092f3f067>\", line 10, in <module>\n    conv2 = tf.layers.conv2d(batch_norm1, filters=32, kernel_size=[5, 5], strides=1, activation='relu', padding='same', name='conv_layer_2')\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/layers/convolutional.py\", line 427, in conv2d\n    return layer.apply(inputs)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py\", line 774, in apply\n    return self.__call__(inputs, *args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/layers/base.py\", line 329, in __call__\n    outputs = super(Layer, self).__call__(inputs, *args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py\", line 703, in __call__\n    outputs = self.call(inputs, *args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/convolutional.py\", line 184, in call\n    outputs = self._convolution_op(inputs, self.kernel)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_ops.py\", line 868, in __call__\n    return self.conv_op(inp, filter)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_ops.py\", line 520, in __call__\n    return self.call(inp, filter)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_ops.py\", line 204, in __call__\n    name=self.name)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_nn_ops.py\", line 956, in conv2d\n    data_format=data_format, dilations=dilations, name=name)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 3414, in create_op\n    op_def=op_def)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 1740, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[50000,32,16,16] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[Node: conv_layer_2/Conv2D = Conv2D[T=DT_FLOAT, data_format=\"NCHW\", dilations=[1, 1, 1, 1], padding=\"SAME\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](batch_norm_1/FusedBatchNorm, conv_layer_2/kernel/read)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "utD13pOh0_xC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "a00e139b-2d9f-451d-c65d-502d7328fd97"
      },
      "cell_type": "code",
      "source": [
        "! nvidia-smi"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fri Aug  3 07:38:35 2018       \r\n",
            "+-----------------------------------------------------------------------------+\r\n",
            "| NVIDIA-SMI 384.111                Driver Version: 384.111                   |\r\n",
            "|-------------------------------+----------------------+----------------------+\r\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
            "|===============================+======================+======================|\r\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\r\n",
            "| N/A   37C    P0    70W / 149W |  10936MiB / 11439MiB |      0%      Default |\r\n",
            "+-------------------------------+----------------------+----------------------+\r\n",
            "                                                                               \r\n",
            "+-----------------------------------------------------------------------------+\r\n",
            "| Processes:                                                       GPU Memory |\r\n",
            "|  GPU       PID   Type   Process name                             Usage      |\r\n",
            "|=============================================================================|\r\n",
            "+-----------------------------------------------------------------------------+\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "0UWEFOjk80uG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import sys"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xGHSR48atdG3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sess.run(train_whole_init)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lqZQ2aTWtmcC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1097a2dd-5dbc-4755-afe4-f0e5c0f114cf"
      },
      "cell_type": "code",
      "source": [
        "sys.getsizeof(iterator)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "56"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "metadata": {
        "id": "rnaa5Yjltutc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "temp = sess.run(image_batch)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "I1KOF1M6t8Xo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5c7535aa-e62d-400c-ed10-d65f2c4f9a9d"
      },
      "cell_type": "code",
      "source": [
        "temp.shape"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(50000, 32, 32, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "metadata": {
        "id": "8Yj6C8g_uFIf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}